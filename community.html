<!DOCTYPE HTML>
<!--
    Spectral by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
  -->
<html>

<head>
  <title>Rucio - Scientific Data Management :: Community</title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <link rel="stylesheet" href="assets/css/main.css" />
  <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
  <link rel="me" href="https://fosstodon.org/@rucio" />
</head>

<body class="landing">

  <!-- Page Wrapper -->
  <div id="page-wrapper">

    <!-- Header -->
    <div id="header-container"></div>

    <section id="community" class="wrapper style5">
      <div class="inner">
        <header class="major">
          <h2>Community</h2>
        </header>

          <h2>ATLAS <a class="icon solid fa-home" href="https://atlas.cern/"></a> <a class="icon brands fa-linkedin" href="https://www.linkedin.com/company/atlas-collaboration/"></a></h2>
	      <p><span class="image left"><img src="images/community/ATLAS.png" alt="" /></span>The ATLAS Experiment is one of the four major experiments at the Large Hadron Collider (LHC). Approaching one Exabyte of data on disk and tape in 2024, ATLAS has always been one of the largest scientific data producers in the world. A well-integrated, resilient, and efficient data management solution is crucial to overall experiment performance and ATLAS was in need of a better solution by the end of LHC Run-1 in 2012. Thus ATLAS invested heavily in the design and development of a new system called Rucio to ensure scalability for future LHC upgrades, allow expressive policies for our data flow needs, connect with our distributed computing infrastructure, automate operational tasks as much as possible, integrate new technologies from the storage and network areas, and combine all this within a modular and extensible architecture. When Rucio was first put in production in 2014, the improvements it brought into ATLAS data management were substantial and also attracted a lot of interest by the wider science community. By now, Rucio manages all ATLAS data, including centrally produced and user-generated, totalling over a billion files distributed across 120+ scientific data centers. Rucio orchestrates daily transfers of tens of Petabytes, ensures optimal usage of our network infrastructure, takes care of seamless integration between scientific storage, supercomputers, and commercial clouds, as well as provides various interfaces to make the daily life of scientists more comfortable. Rucio has also matured into an open-source community project, and we are extremely happy about its continued success. The idea of a common data management system, in use by many communities with similar needs, is a guiding principle for us and we will continue to invest into our shared future.</p>

          <h2>CMS <a class="icon solid fa-home" href="https://cms.cern/"></a></h2>
	      <p><span class="image right"><img src="images/community/CMS.png" alt="" /></span>The CMS Collaboration brings together members of the particle physics community from across the globe in a quest to advance humanity’s knowledge of the very basic laws of our Universe. CMS has over 4000 particle physicists, engineers, computer scientists, technicians and students from around 240 institutes and universities from more than 50 countries.</p>
          <p>The collaboration operates and collects data from the Compact Muon Solenoid, one of two general-purpose particle detectors at CERN’s Large Hadron Collider. Data collected by CMS are distributed to CMS institutions in over forty countries for physics analysis.</p>
          <p>In 2018, CMS embarked on a process to select a new data management solution. The previous solution was over a decade old, difficult to maintain, and would not easily adapt to the data rates and technologies used for data transfers in the HL-LHC era. As a result of this selection process, CMS decided to adopt Rucio which was, at the time, used by on major experiment and a couple of smaller experiments.</p>
          <p>This choice has been a good one for CMS, allowing them to no longer operate a service at each of more than 50 data sites, to scale easily to new rates of data transfer, and to adopt new technologies for data transfer as needed. CMS aggregate data rates, managed by Rucio, regularly top 40 GB/s and have been proven to reach 100 GB/s.</p>

          <h2>ESCAPE <a class="icon solid fa-home" href="https://projectescape.eu"></a> <a class="icon brands fa-twitter" href="https://twitter.com/ESCAPE_EU"></a>  <a class="icon brands fa-linkedin" href="https://www.linkedin.com/company/projectescape/"></a> <a class="icon brands fa-youtube" href="https://www.youtube.com/channel/UC05braEQdP2rCSUamHm9I_Q"></a> </h2>
          <p><span class="image left"><img src="images/community/ESCAPE.png" alt="" /></span>ESCAPE (European Science Cluster of Astronomy & Particle physics ESFRI research infrastructures) brings together the astronomy, astroparticle and particle physics communities. Establishing a collaborative cluster of next generation ESFRI facilities to implement a functional link between the concerned ESFRIs and the European Open Science Cloud (EOSC).</p>
		  <p>ESCAPE aims to produce versatile solutions, with great potential for discovery, to support the implementation of EOSC thanks to open data management, cross-border and multi-disciplinary open environment, according to FAIR (Findable, Accessible, Interoperable and Reusable) principles. Key outputs of  ESCAPE collaboration include Data Infrastructure for Open Science, Science Analysis Platform, Software Repositories, Citizen Science activities and  training/education activities.</p>
		  <p>Since 2023 ESCAPE evolved to a long standing Collaboration Agreement to continue addressing the challenges shared by its partners and the relevant communities for the implementation of open-science practices and the management of FAIR digital research objects into the core operation of ESFRI projects, landmark projects and other relevant world class RIs. These challenges are technical, operational, sociological and scientific.</p>
		  <p>ESCAPE has chosen Rucio as its data management solution. This move signifies a significant step towards enhancing and streamlining the data distribution, management, and analysis processes within the scientific community. Since then <a href="https://www.ctao.org">CTAO</a>, <a href="https://www.skao.int">SKA</a> and the <a href="https://rubinobservatory.org">Vera C. Rubin Observatory</a> have officially selected Rucio as their data management solution.</p>
	
          <h2>Belle II <a class="icon solid fa-home" href="https://www.belle2.org/"></a> <a class="icon brands fa-facebook" href="https://www.facebook.com/belle2collab"></a> <a class="icon brands fa-twitter" href="https://x.com/belle2collab"></a></h2>
	      <p><span class="image right"><img src="images/community/BelleII.png" alt="" /></span>The Belle II experiment is a particle physics experiment located at High Energy Accelerator Research Organization (KEK) in Tsukuba, Japan. It is the successor to the Belle experiment operated from 1999 to 2010 and expected to collect around 50 times more data. In order to be able to manage this huge volume of data, Belle II decided to move to Rucio that has demonstrated its capabilities at scales much higher than the current volume managed by Belle II (over hundred million files, more than 25 Petabytes) and that will be able to cope with the volume expected in the future.</p>
		  <p>Since January 2021, Rucio is responsible for managing all the data produced across the data centers used by Belle II over their full lifecycle. In particular it ensures the proper replication of all the data according to the replication policies of Belle II. Rucio fits very well to Belle II needs and it allowed to simplify and automate many tasks.</p>
		  <p>At the beginning of 2024, Rucio was also chosen to become the official metadata service of Belle II in the near future after a careful evaluation. We expect to gain immediate benefits from it when this functionality is enabled in production.</p> 

          <h2>DUNE <a class="icon solid fa-home" href="https://www.dunescience.org"></a> <a class="icon brands fa-facebook" href="https://www.facebook.com/DUNEscience/"></a></h2>
	      <p><span class="image left"><img src="images/community/DUNE.jpeg" alt="" /></span>The Deep Underground Neutrino Experiment (DUNE) is an international experiment exploring the origins of matter, the ordering of neutrino masses, and potentially the formation of black holes through the observation of neutrinos from supernova core collapse. DUNE consists of two neutrino detectors situated underground with roughly 1300 km between the near and far detectors. The near detector will be located on site at Fermi National Accelerator Laboratory with the far detector located 4850 ft underground at Sanford Underground Research Facility in Lead, SD. These detectors will be exposed to the neutrinos created by the Long Baseline Neutrino Facility at Fermilab.</p>
          <p>The raw data produced from the DUNE detectors are unique compared with other HEP experiments for both readout of neutrino interactions and observation of astrophysical events. The DUNE far detectors will consist of several large Liquid Argon Time Projection Chambers filled with approximately 17 kilotons of liquid argon. The far detector data produced from a single trigger of DAQ can vary in size from 100 MB for neutrino interaction candidates to larger than 100 TB for supernova burst candidates. The cataloging and replication of the large data volumes poses an interesting challenge for DUNE when trying to optimize workflows with large I/O requirements based upon these large volume trigger records. From the beginning, DUNE chose Rucio to manage data distribution, replication, and removal. As well, DUNE has become an active part of the Rucio development community as the experiment brings unique needs to data management.</p>

          <h2>SKAO <a class="icon solid fa-home" href="https://www.skao.int"></a></h2>
	      <p><span class="image right"><img src="images/community/SKAO.png" alt="" /></span>The SKA Observatory, or SKAO for short, is an intergovernmental organisation bringing together nations from around the world. Its mission is to build and operate cutting-edge radio telescopes to transform our understanding of the Universe, and deliver benefits to society through global collaboration and innovation. The observatory has a global footprint and consists of the SKAO Global Headquarters in the UK, the SKAO’s two telescopes at radio-quiet sites in South Africa and Australia, and associated facilities to support the operations of the telescopes.  Once in operation, the SKAO will be one global observatory operating two telescopes across three continents on behalf of its Member States and partners. </p>
          <p>The SKAO's staggering data rates (710 PB/yr of science data) mean that a simple "download here" option will not be appropriate – functionality to find, assess, manipulate and visualise SKA Data products needs to be made available to a global user community on shared computational resources. To achieve this, the Observatory will be supported by a global network of SKA Regional Centres, or SRCs, distributed around the world in its member states. The Observatory is working with the international science community to develop these SRCs as a collaborative ecosystem: the SRCNet. Each SRC in the SRCNet will provide access to data products, platforms for advanced scientific analysis, and user support and training for astronomers using data generated by the SKA telescopes. To help manage and orchestrate the movement and replication of this data across the SRCNet, the SKAO has adopted Rucio for its first version, 0.1.</p>

      </div>

      <!-- CTA -->
      <section id="cta" class="wrapper style5">
        <div class="inner">
          <header>
            <h2>Get in touch</h2>
            <p>We are always happy to chat. You can drop us a mail and we will reply as quickly as possible.
            </p>
          </header>
          <ul class="actions vertical">
            <li><a href="contact.html" class="button special primary">Contact us</a></li>
          </ul>

        </div>
      </section>

      <!-- Footer -->
      <div id="footer-container"></div>

    </section>
  </div>
	<!-- Scripts -->
    <script src="assets/js/header.js"></script>
    <script src="assets/js/footer.js"></script>
    <script src="assets/js/scripts.js"></script>

</body>

</html>
